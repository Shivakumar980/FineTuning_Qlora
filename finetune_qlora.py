# -*- coding: utf-8 -*-
"""Finetune_Qlora.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Iby5vYM5iubuJb1qH34FAaTQoWGAfUys
"""

!pip install -q bitsandbytes==0.45.2
!pip install -q accelerate==0.30.1
!pip install -q transformers==4.41.0
!pip install -q peft==0.10.0
!pip install -q datasets==2.18.0
!pip install -q trl==0.9.6
!pip install -q scipy==1.11.4

import torch
import transformers
import peft
import bitsandbytes as bnb
import accelerate
import trl
import datasets

print("‚úÖ Package versions:")
print(f"  torch: {torch.__version__}")
print(f"  transformers: {transformers.__version__}")
print(f"  peft: {peft.__version__}")
print(f"  bitsandbytes: {bnb.__version__}")
print(f"  accelerate: {accelerate.__version__}")
print(f"  trl: {trl.__version__}")
print(f"  datasets: {datasets.__version__}")
print(f"\n‚úÖ CUDA: {torch.cuda.is_available()}")
print(f"‚úÖ GPU: {torch.cuda.get_device_name(0)}")

from huggingface_hub import notebook_login

print("üîê Hugging Face Authentication Required")
print("="*50)
print("Steps:")
print("1. Go to: https://huggingface.co/settings/tokens")
print("2. Create a token (Read access is enough)")
print("3. Accept Llama-2 license: https://huggingface.co/meta-llama/Llama-2-7b-hf")
print("4. Paste your token below")
print("="*50)

notebook_login()

print("‚úÖ Authentication successful!")

# Uninstall and reinstall compatible versions
!pip uninstall -y transformers trl
!pip install transformers==4.41.0
!pip install trl==0.8.6

print("‚úÖ Packages updated - now run Cell 3 again")

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer
from datasets import Dataset

# Configuration - Llama-2-7B
model_name = "meta-llama/Llama-2-7b-hf"
new_model_name = "llama-2-7b-qlora-interview"

print(f"üîÑ Loading model: {model_name}")
print("This will take 3-5 minutes...")

# 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

# Load base model
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)
model.config.use_cache = False
model.config.pretraining_tp = 1

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# Prepare model for k-bit training
model = prepare_model_for_kbit_training(model)

# LoRA configuration
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
)

# Get PEFT model
model = get_peft_model(model, peft_config)

# Print trainable parameters
print("\n" + "="*50)
model.print_trainable_parameters()
print("="*50)
print("‚úÖ Llama-2-7B loaded and configured for QLoRA training!")

# Create interview Q&A dataset
dataset_dict = {
    "instruction": [
        "Explain the difference between supervised and unsupervised learning",
        "What is overfitting in machine learning?",
        "Explain gradient descent",
        "What is a neural network?",
        "Describe the bias-variance tradeoff",
        "What is cross-validation?",
        "Explain backpropagation",
        "What is regularization?",
        "What is the purpose of activation functions?",
        "Explain the concept of learning rate",
        "What is batch normalization?",
        "Describe the difference between L1 and L2 regularization",
        "What is transfer learning?",
        "Explain the vanishing gradient problem",
        "What is dropout in neural networks?",
        "What are convolutional neural networks?",
        "Explain recurrent neural networks",
        "What is attention mechanism in deep learning?",
        "Describe the transformer architecture",
        "What is fine-tuning in machine learning?",
    ],
    "output": [
        "Supervised learning uses labeled data where the model learns from input-output pairs. Unsupervised learning works with unlabeled data to find patterns and structures without predefined outputs.",
        "Overfitting occurs when a model learns the training data too well, including noise and outliers, resulting in poor generalization to new, unseen data.",
        "Gradient descent is an optimization algorithm that iteratively adjusts model parameters by moving in the direction of steepest descent of the loss function to minimize error.",
        "A neural network is a computational model inspired by biological neurons, consisting of interconnected layers of nodes that process and transform input data to produce predictions.",
        "The bias-variance tradeoff describes the balance between a model's ability to minimize bias (underfitting) and variance (overfitting) to achieve optimal predictive performance.",
        "Cross-validation is a technique to assess model performance by splitting data into multiple folds, training on some and validating on others, to ensure robust evaluation.",
        "Backpropagation is the algorithm used to train neural networks by computing gradients of the loss function with respect to weights using the chain rule, then updating weights accordingly.",
        "Regularization techniques like L1 and L2 add penalty terms to the loss function to prevent overfitting by constraining model complexity and reducing parameter magnitudes.",
        "Activation functions introduce non-linearity into neural networks, allowing them to learn complex patterns. Common examples include ReLU, sigmoid, and tanh.",
        "Learning rate controls the step size during gradient descent optimization. A proper learning rate balances training speed with convergence stability.",
        "Batch normalization normalizes layer inputs to stabilize training, reduce internal covariate shift, and enable higher learning rates for faster convergence.",
        "L1 regularization adds absolute value of weights as penalty, promoting sparsity. L2 regularization uses squared weights, distributing penalty across all parameters more evenly.",
        "Transfer learning leverages pre-trained models on large datasets and fine-tunes them for specific tasks, reducing training time and data requirements.",
        "The vanishing gradient problem occurs when gradients become extremely small during backpropagation through deep networks, preventing effective weight updates in early layers.",
        "Dropout randomly deactivates neurons during training to prevent co-adaptation, forcing the network to learn robust features and reducing overfitting.",
        "Convolutional Neural Networks (CNNs) use convolution operations to automatically learn spatial hierarchies of features from images, with layers detecting edges, textures, and complex patterns.",
        "Recurrent Neural Networks (RNNs) process sequential data by maintaining hidden states that capture information from previous time steps, enabling modeling of temporal dependencies.",
        "Attention mechanism allows models to focus on relevant parts of input when making predictions, weighing the importance of different elements dynamically rather than treating all equally.",
        "The transformer architecture uses self-attention mechanisms to process sequences in parallel, eliminating recurrence while capturing long-range dependencies through multi-head attention and positional encodings.",
        "Fine-tuning adapts a pre-trained model to a specific task by continuing training on domain-specific data, leveraging learned representations while specializing for the target application.",
    ]
}

# Convert to Hugging Face dataset
dataset = Dataset.from_dict(dataset_dict)

# Format dataset for instruction tuning (Llama-2 format)
def format_instruction(sample):
    return f"<s>[INST] {sample['instruction']} [/INST] {sample['output']} </s>"

dataset = dataset.map(lambda x: {"text": format_instruction(x)})

print(f"‚úÖ Dataset prepared with {len(dataset)} training examples")
print("\nSample formatted text:")
print("="*50)
print(dataset[0]['text'][:200] + "...")
print("="*50)

# Training arguments - optimized for Llama-2-7B on T4
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=2,  # Reduced for 7B model
    gradient_accumulation_steps=4,   # Increased to maintain effective batch size
    learning_rate=2e-4,
    fp16=True,
    save_steps=50,
    logging_steps=5,
    save_total_limit=2,
    warmup_ratio=0.03,
    lr_scheduler_type="cosine",
    optim="paged_adamw_8bit",
    report_to="none",
    max_steps=-1,
)

# Initialize Trainer
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=peft_config,
    dataset_text_field="text",
    max_seq_length=512,
    tokenizer=tokenizer,
    args=training_args,
)

print("‚úÖ Trainer configured!")
print(f"Total training steps: {len(dataset) * training_args.num_train_epochs // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}")
print(f"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")

# Start training
print("üöÄ Starting QLoRA training on Llama-2-7B...")
print("This will take approximately 25-35 minutes on T4 GPU")
print("="*50)

trainer.train()

print("\n" + "="*50)
print("‚úÖ Training complete!")
print("="*50)

# Save the fine-tuned adapter weights
output_dir = "./llama-2-7b-qlora-finetuned"
trainer.model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

print(f"‚úÖ Fine-tuned adapter saved to: {output_dir}")

# Check saved files
import os
print("\nSaved files:")
for file in os.listdir(output_dir):
    print(f"  - {file}")

# Quick inference test
print("üß™ Testing fine-tuned model...")
print("="*50)

# Test prompt
test_prompt = "<s>[INST] What is deep learning and how does it differ from traditional machine learning? [/INST]"

# Tokenize
inputs = tokenizer(test_prompt, return_tensors="pt").to("cuda")

# Generate
outputs = model.generate(
    **inputs,
    max_new_tokens=200,
    temperature=0.7,
    top_p=0.9,
    do_sample=True,
    repetition_penalty=1.1,
)

# Decode and print
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
print("="*50)
print("‚úÖ Model is responding!")

import os

print("üîç Searching for saved files...")

# Check current directory structure
print("\nDirectories in /content:")
for item in os.listdir("/content"):
    if os.path.isdir(f"/content/{item}"):
        print(f"  üìÅ {item}")

# Search for adapter files
print("\nüîç Searching for adapter_config.json...")
!find /content -name "adapter_config.json" 2>/dev/null

print("\nüîç Searching for checkpoint directories...")
!find /content -type d -name "checkpoint-*" 2>/dev/null

print("\nüîç All .json files in /content:")
!find /content -name "*.json" -type f 2>/dev/null | head -20

from peft import PeftModel
import torch
import os
from transformers import AutoModelForCausalLM, AutoTokenizer
import gc
from google.colab import userdata

print("üîÑ Merging LoRA adapters (CPU-based approach)...")

# Get token
hf_token = userdata.get('HF_token')

# Paths
model_name = "meta-llama/Llama-2-7b-hf"
output_dir = "/content/llama-2-7b-qlora-finetuned"

# Clear memory
gc.collect()
torch.cuda.empty_cache()

print("Loading base model on CPU (this takes 5-7 min)...")
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map={"": "cpu"},  # Load entirely on CPU
    trust_remote_code=True,
    token=hf_token,
)

print("Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)

print("Loading LoRA adapters on CPU...")
model_with_adapters = PeftModel.from_pretrained(
    base_model,
    output_dir,
    device_map={"": "cpu"},  # Load on CPU
)

print("üîÑ Merging on CPU... (this takes 3-5 min)")
merged_model = model_with_adapters.merge_and_unload()

print("üíæ Saving merged model...")
final_output = "/content/llama-2-7b-merged-vllm"
merged_model.save_pretrained(
    final_output,
    safe_serialization=True,
    max_shard_size="2GB"
)
tokenizer.save_pretrained(final_output)

print(f"\n‚úÖ Merged model saved to: {final_output}")
!du -sh {final_output}

# Cleanup
del base_model, model_with_adapters, merged_model
gc.collect()

print("\n‚úÖ Merge complete! Model ready for vLLM deployment.")

from google.colab import drive
import os

print("üì¶ Uploading merged model to Google Drive...")

# Mount Drive if not already mounted
if not os.path.exists("/content/drive"):
    drive.mount('/content/drive')

# Create directory
drive_path = '/content/drive/MyDrive/llama-models'
os.makedirs(drive_path, exist_ok=True)

# Copy merged model to Drive
print("üì§ Copying 13GB model to Google Drive...")
print("This will take 10-15 minutes...")

!cp -r /content/llama-2-7b-merged-vllm /content/drive/MyDrive/llama-models/

print("\n‚úÖ Model uploaded to Google Drive!")
print(f"üìÇ Location: MyDrive/llama-models/llama-2-7b-merged-vllm")

# Verify
!du -sh /content/drive/MyDrive/llama-models/llama-2-7b-merged-vllm

print("\n" + "="*50)
print("‚úÖ PHASE 1 COMPLETE!")
print("="*50)
print("\nNext steps:")
print("1. Download model from Google Drive to your MacBook")
print("2. Upload to AWS S3")
print("3. Deploy on EKS with vLLM + Prometheus")
print("="*50)

# Install boto3
!pip install boto3 -q

print("‚úÖ boto3 installed!")

import boto3
from pathlib import Path
import os
from tqdm import tqdm

# AWS Configuration
S3_BUCKET = f"llama-qlora-{int(__import__('time').time())}"
REGION = "us-east-1"

# Model path in Google Drive
MODEL_PATH = Path("/content/drive/MyDrive/llama-models/llama-2-7b-merged-vllm")

print(f"üöÄ Uploading from Google Drive to S3...")
print(f"Bucket: {S3_BUCKET}")

# Initialize S3 client
s3_client = boto3.client(
    's3',
    region_name=REGION,
    aws_access_key_id=AWS_ACCESS_KEY,
    aws_secret_access_key=AWS_SECRET_KEY
)

# Create bucket
print(f"\nüì¶ Creating S3 bucket...")
try:
    s3_client.create_bucket(Bucket=S3_BUCKET)
    print(f"‚úÖ Bucket created: {S3_BUCKET}")
except Exception as e:
    print(f"Error: {e}")

# Get all files
files = [f for f in MODEL_PATH.rglob('*') if f.is_file()]
total_size = sum(f.stat().st_size for f in files)

print(f"\nüì§ Uploading {len(files)} files ({total_size / (1024**3):.2f} GB)")
print("This will take 5-10 minutes (Colab has fast upload speeds)")

# Upload with progress
with tqdm(total=len(files), desc="Uploading", unit="file") as pbar:
    for file_path in files:
        relative_path = file_path.relative_to(MODEL_PATH)
        s3_key = f"llama-2-7b-merged-vllm/{relative_path}"

        s3_client.upload_file(
            str(file_path),
            S3_BUCKET,
            s3_key
        )
        pbar.update(1)

print(f"\n‚úÖ Upload complete!")
print(f"S3 location: s3://{S3_BUCKET}/llama-2-7b-merged-vllm/")

# Save bucket name
print(f"\nüìù Bucket name: {S3_BUCKET}")
print("Copy this bucket name for EKS deployment!")